#
"""
Client side - generating data files
- intro
- build and set context
- persist source path
- persist destination path
- generate file in source path
- compress files in source path
- send compressed file
"""

import os
import json

import pendulum

from airflow import DAG
from airflow.decorators import task
from airflow.operators.bash import BashOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.http.operators.http import SimpleHttpOperator

# use to fail a task
from airflow.exceptions import AirflowFailException

# use to fail a task, but with option for retries later
#from airflow import AirflowException

from workflow_lib import gen_batch_id, get_dag_context, display_dir_content, generate_client_files, copy_files
from bash_templates import compress_bash_cmd_tmpl



# A DAG represents a workflow, a collection of tasks #
with DAG(dag_id="lala_ctc_generating_data", 
    start_date=pendulum.datetime(2022, 11, 27, tz="UTC"), 
    schedule=None,
    default_args={
        "provide_context": True,
    }) as dag:

    # Tasks are represented as operators
    intro = BashOperator(task_id="intro", bash_command='echo "Generate random amount of file"')

    @task()
    def context(ti=None):
        context = get_dag_context(__file__)
        ti.xcom_push(key="file_criteria", value=context['file_criteria'])

        client_id = "lala"
        ti.xcom_push(key="client_id", value=client_id)

        project_id = "ctc"
        ti.xcom_push(key="project_id", value=project_id)

        batch_id = gen_batch_id(6)
        ti.xcom_push(key="batch_id", value=batch_id)


        ti.xcom_push(key="passed_context", value=f"{client_id}-{project_id}-{batch_id}")

        batch_src_path = os.path.join(os.environ.get("CLIENT_BATCH_ROOT"), batch_id)
        ti.xcom_push(key="batch_src_path", value=batch_src_path)

        targz_file = f"lala_ctc_{batch_id}.tar.gz"
        ti.xcom_push(key="targz_file", value=targz_file)

        batch_dest_path = os.path.join(os.environ.get("INGESTION_ROOT"), client_id, project_id, batch_id)
        ti.xcom_push(key="batch_dest_path", value=batch_dest_path)

    @task()
    def create_batch_folder(ti=None):

        batch_root = os.environ.get("CLIENT_BATCH_ROOT")
        root_exists = os.path.isdir(batch_root)
        print(f"batch_root: {batch_root} (exists? {root_exists})")
        if not root_exists:
            raise AirflowFailException("Batch root does not exist!!!")

        batch_src_path = ti.xcom_pull(task_ids="context", key="batch_src_path")
        batch_path_exists = os.path.isdir(batch_src_path)
        print(f"batch_src_path: {batch_src_path} (exists? {batch_path_exists})")
        if batch_path_exists:
            raise AirflowFailException("Batch path already exists.  Should always be new and not exist at this point!!!")

        print(f"creating batch path: {batch_src_path}")
        os.mkdir(batch_src_path, 0o777)
        display_dir_content(batch_root)

        new_batch_path_exists = os.path.isdir(batch_src_path)
        if not new_batch_path_exists:
            raise AirflowFailException("Batch path was supposed to be created now and is missing!!!")

        return batch_src_path

    @task()
    def create_ingestion_folder(ti=None):
        """
        This task would normally be a dynamic task on the receiving dag of data generated by this dag
        """

        # verify ingestion root, else exit
        ingest_root = os.environ.get("INGESTION_ROOT")
        root_exists = os.path.isdir(ingest_root)
        print(f"batch_root: {ingest_root} (exists? {root_exists})")
        if not root_exists:
            raise AirflowFailException("Ingestion root does not exist!!!")

        # create ingestion batch root
        #batch_dest_path = os.path.join(ingest_root, client_id, project_id, batch_id)
        batch_dest_path = ti.xcom_pull(task_ids="context", key="batch_dest_path")
        batch_path_exists = os.path.isdir(batch_dest_path)
        print(f"batch_dest_path: {batch_dest_path} (exists? {batch_path_exists})")
        if batch_path_exists:
            raise AirflowFailException("Ingestion Batch path already exists.  Should always be new and not exist at this point!!!")


        print(f"creating batch path: {batch_dest_path}")
        os.mkdir(batch_dest_path, 0o777)
        display_dir_content(batch_dest_path)


        new_batch_path_exists = os.path.isdir(batch_dest_path)
        if not new_batch_path_exists:
            raise AirflowFailException("Ingestion Batch path was supposed to be created now and is missing!!!")

        return batch_dest_path

    @task()
    def generate_files(ti=None):
        # get context
        client_id = ti.xcom_pull(task_ids="context", key="client_id")
        project_id = ti.xcom_pull(task_ids="context", key="project_id")
        batch_id = ti.xcom_pull(task_ids="context", key="batch_id")
        file_criteria = ti.xcom_pull(task_ids="context", key="file_criteria")

        generate_client_files(client_id, project_id, batch_id, file_criteria)
        
    
    # Tasks are represented as operators
    compress = BashOperator(
        task_id="compress_file",
        bash_command=compress_bash_cmd_tmpl)
        
    @task()
    def send_files(ti=None):
        
        targz_file = ti.xcom_pull(task_ids="context", key="targz_file")
        print(f"compress_file: {targz_file}")

        batch_src_path = ti.xcom_pull(task_ids="context", key="batch_src_path")
        print(f"batch_src_path: {batch_src_path}")
        
        batch_dest_path = ti.xcom_pull(task_ids="context", key="batch_dest_path")
        print(f"batch_dest_path: {batch_dest_path}")

        print("\n\n### Temporarily copy directly from client data source to platform ingestion area")
        copy_files(batch_src_path, batch_dest_path, [targz_file,])

        display_dir_content(batch_dest_path)


    trigger_dag = TriggerDagRunOperator(
        task_id="trigger_platform_dag_old",
        trigger_dag_id="tenant__lala__ctc__etl",
        conf={'context':'{{ ti.xcom_pull(task_ids="context", key="passed_context") }}'}
    )

    """
    http_trigger_dag = SimpleHttpOperator(
        task_id="trigger_platform_dag",
        http_conn_id='airflow-api',
        endpoint='/api/v1/dags/tenant__lala__ctc__etl/dagRuns',
        method='POST',
        headers={'Content-Type': 'application/json'},
        data=json.dumps({'context':'{{ ti.xcom_pull(task_ids="context", key="passed_context") }}'})
    )
    """

    # Set dependencies between tasks
    intro >> context() >> create_batch_folder() >> create_ingestion_folder() >> generate_files() >> compress >> send_files() >> trigger_dag

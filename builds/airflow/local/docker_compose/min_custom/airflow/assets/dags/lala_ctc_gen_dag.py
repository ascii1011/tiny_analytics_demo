#
"""
Client side - generating data files
- intro, bash str
- get context (extract context from file name for generating files and data, dag_id, project_id, workflow type)
- create batch id
- create
"""

import os
from datetime import datetime

from airflow import DAG
from airflow.decorators import task
from airflow.operators.bash import BashOperator

# use to fail a task
from airflow.exceptions import AirflowFailException

# use to fail a task, but with option for retries later
#from airflow import AirflowException

from workflow_lib import gen_batch_id, get_dag_context, display_dir_content, generate_client_files, copy_files
from bash_templates import compress_bash_cmd_tmpl

# A DAG represents a workflow, a collection of tasks #
with DAG(dag_id="lala_ctc_generating_data", start_date=datetime(2022, 11, 9), schedule=None) as dag:

    # Tasks are represented as operators
    intro = BashOperator(task_id="intro", bash_command='echo "Generate random amount of file"')

    @task
    def client():
        return get_dag_context(__file__)

    @task()
    def batch_id():
        return gen_batch_id(6)

    @task()
    def create_batch_folder(ti=None):
        context = ti.xcom_pull(task_ids="client")
        batch_id = ti.xcom_pull(task_ids="batch_id")

        batch_root = os.environ.get("CLIENT_BATCH_ROOT")
        root_exists = os.path.isdir(batch_root)
        print(f"batch_root: {batch_root} (exists? {root_exists})")
        if not root_exists:
            raise AirflowFailException("Batch root does not exist!!!")


        batch_src_path = os.path.join(os.environ.get("CLIENT_BATCH_ROOT"), batch_id)
        batch_path_exists = os.path.isdir(batch_src_path)
        print(f"batch_src_path: {batch_src_path} (exists? {batch_path_exists})")
        if batch_path_exists:
            raise AirflowFailException("Batch path already exists.  Should always be new and not exist at this point!!!")


        print(f"creating batch path: {batch_src_path}")
        os.mkdir(batch_src_path, 0o777)
        display_dir_content(batch_root)


        new_batch_path_exists = os.path.isdir(batch_src_path)
        if not new_batch_path_exists:
            raise AirflowFailException("Batch path was supposed to be created now and is missing!!!")

        return batch_src_path

    @task()
    def create_ingestion_folder(ti=None):
        """
        This task would normally be a dynamic task on the receiving dag of data generated by this dag
        """

        # get context
        context = ti.xcom_pull(task_ids="client")
        client_id = context["client_id"]
        project_id = context["project_id"]

        batch_id = ti.xcom_pull(task_ids="batch_id")

        # verify ingestion root, else exit
        ingest_root = os.environ.get("INGESTION_ROOT")
        root_exists = os.path.isdir(ingest_root)
        print(f"batch_root: {ingest_root} (exists? {root_exists})")
        if not root_exists:
            raise AirflowFailException("Ingestion root does not exist!!!")

        # create ingestion batch root
        batch_dest_path = os.path.join(ingest_root, client_id, project_id, batch_id)
        batch_path_exists = os.path.isdir(batch_dest_path)
        print(f"batch_dest_path: {batch_dest_path} (exists? {batch_path_exists})")
        if batch_path_exists:
            raise AirflowFailException("Ingestion Batch path already exists.  Should always be new and not exist at this point!!!")


        print(f"creating batch path: {batch_dest_path}")
        os.mkdir(batch_dest_path, 0o777)
        display_dir_content(batch_dest_path)


        new_batch_path_exists = os.path.isdir(batch_dest_path)
        if not new_batch_path_exists:
            raise AirflowFailException("Ingestion Batch path was supposed to be created now and is missing!!!")

        return batch_dest_path

    @task()
    def generate_files(ti=None):
        # get context
        context = ti.xcom_pull(task_ids="client")
        client_id = context["client_id"]
        project_id = context["project_id"]
        file_criteria = context["file_criteria"]

        batch_id = ti.xcom_pull(task_ids="batch_id")
        generate_client_files(client_id, project_id, batch_id, file_criteria)
        
    
    # Tasks are represented as operators
    compress = BashOperator(
        task_id="compress_file",
        bash_command=compress_bash_cmd_tmpl)
        
    @task()
    def send_files(ti=None):
        compress_file = ti.xcom_pull(task_ids="compress_file")

        batch_src_path = os.path.join(os.environ.get("CLIENT_BATCH_ROOT"), batch_id)
        print(f"batch_src_path: {batch_src_path}")
        
        batch_dest_path = os.path.join(os.environ.get("INGESTION_ROOT"), client_id, project_id, batch_id)
        print(f"batch_dest_path: {batch_dest_path}")

        print("\n\n### Temporarily copy directly from client data source to platform ingestion area")
        copy_files(batch_src_path, batch_dest_path, [compress_file,])

        display_dir_content(batch_dest_path)

    # Set dependencies between tasks
    intro >> client() >> batch_id() >> create_batch_folder() >> create_ingestion_folder() >> generate_files() >> compress
